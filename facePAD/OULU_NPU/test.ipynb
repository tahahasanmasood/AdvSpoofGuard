{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from torchvision import transforms, models\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_label_live = 1\n",
    "class_label_print_attack_1 = 2\n",
    "class_label_print_attack_2 = 3\n",
    "class_label_display_attack_1 = 4\n",
    "class_label_display_attack_2 = 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_test_live = '/OULU_NPU/test/real'\n",
    "data_path_test_print_attack_1 = '/OULU_NPU/test/attack_sep/print1'\n",
    "data_path_test_print_attack_2 = '/OULU_NPU/test/attack_sep/print2'\n",
    "data_path_test_display_attack_1 = '/OULU_NPU/test/attack_sep/display1'\n",
    "data_path_test_display_attack_2 = 'OULU_NPU/test/attack_sep/display2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_samples(path, class_label, transform): #Select N frames returned from read_all_frames and assign labels to all samples of same class\n",
    "        frames = read_all_frames(path)\n",
    "        total_frames = list(range(0, frames.shape[0], 1))\n",
    "        selected_samples = 5\n",
    "        selected_frame = total_frames[selected_samples]\n",
    "        samples =[]\n",
    "        # Assign the same class label to all samples\n",
    "        label = class_label\n",
    "        samples =(transform(frames[selected_frame].squeeze()), label)     \n",
    "        return samples\n",
    "\n",
    "def read_all_frames(video_path): # reads all frames from a particular video and converts them to PyTorch tensors.\n",
    "    frame_list = []\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    success = True\n",
    "    while success:\n",
    "        success, frame = video.read()\n",
    "        if success:\n",
    "            frame = cv2.resize(frame, (256, 256), interpolation=cv2.INTER_AREA) #framesize kept 256 x 256\n",
    "            frame_list.append(frame)\n",
    "    frame_list = np.array(frame_list)\n",
    "    return frame_list\n",
    "\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, data_path, class_label):\n",
    "        self.data_path = data_path #path for directory containing video files\n",
    "        self.video_files = [file for file in os.listdir(data_path) if file.endswith('.avi')]\n",
    "        self.class_label = class_label #manually assign class_label for your desired class while loading\n",
    "        self.data_length = len(self.video_files) \n",
    "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    def __len__(self): # returns the total number of samples in the dataset\n",
    "        return self.data_length\n",
    "\n",
    "    def __getitem__(self, idx): # loads and returns a sample from the dataset at the given index\n",
    "        file = self.video_files[idx]\n",
    "        path = os.path.join(self.data_path, file)\n",
    "        frames= load_samples(path, self.class_label, self.transform)\n",
    "\n",
    "        return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_live = VideoDataset(data_path_test_live, class_label_live)\n",
    "test_dataset_print_attack_1 = VideoDataset(data_path_test_print_attack_1, class_label_print_attack_1)\n",
    "test_dataset_print_attack_2 = VideoDataset(data_path_test_print_attack_2, class_label_print_attack_2)\n",
    "test_dataset_display_attack_1 = VideoDataset(data_path_test_display_attack_1, class_label_display_attack_1)\n",
    "test_dataset_display_attack_2 = VideoDataset(data_path_test_display_attack_2, class_label_display_attack_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_test_dataset = ConcatDataset([test_dataset_live, test_dataset_print_attack_1, test_dataset_print_attack_2, test_dataset_display_attack_1, test_dataset_display_attack_2])\n",
    "concatenated_test_loader = DataLoader(concatenated_test_dataset, batch_size=64, shuffle=False, pin_memory=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set size: 1800\n"
     ]
    }
   ],
   "source": [
    "# Print dataset sizes\n",
    "print(f\"Test set size: {len(concatenated_test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/taha/anaconda3/envs/pt_fpad/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/taha/anaconda3/envs/pt_fpad/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained MobileNetV2\n",
    "model = models.mobilenet_v2(pretrained=True)\n",
    "model.classifier[1] = nn.Linear(in_features=1280, out_features=2) #default in_features =1280, out_features = 1000\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Define the path to your saved model file\n",
    "model_path = '/OULUNPU_best_model.pth' # just change the model before and after adversarial training\n",
    "\n",
    "# Load the saved model\n",
    "checkpoint = torch.load(model_path)\n",
    "\n",
    "# Load the model's state dictionary\n",
    "model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.17%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the test set\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    \n",
    "    test_cat_labels = torch.empty(0, dtype=torch.int64, device=device)\n",
    "    test_predicted_cat_labels = torch.empty(0, dtype=torch.int64, device=device)\n",
    "    test_model_op_cat = torch.empty(0, dtype=torch.int64, device=device)\n",
    "\n",
    "    for test_images, test_labels in concatenated_test_loader:\n",
    "        test_images, test_labels = test_images.to(device), test_labels.to(device)\n",
    "        test_model_op = model(test_images)\n",
    "        _, test_predicted = torch.max(test_model_op, 1)\n",
    "        test_correct += (test_predicted == test_labels).sum().item() \n",
    "        test_total += test_labels.size(0)\n",
    "\n",
    "        test_cat_labels = torch.cat((test_cat_labels, test_labels))\n",
    "        test_predicted_cat_labels = torch.cat((test_predicted_cat_labels, test_predicted))\n",
    "        test_model_op_cat = torch.cat((test_model_op_cat, test_model_op))\n",
    "\n",
    "    test_accuracy = test_correct / test_total * 100  \n",
    "    print(f'Test Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cat_labels_cpu = test_cat_labels.cpu()\n",
    "test_predicted_cat_labels_cpu = test_predicted_cat_labels.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "softmax_output = F.softmax(test_model_op_cat, dim=1)\n",
    "\n",
    "test_model_op_cat_second_column = softmax_output[:, 1]\n",
    "\n",
    "test_model_op_cat_second_column_cpu = test_model_op_cat_second_column.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subtracted because our labels are opposite as compared to oulumetrics\n",
    "\n",
    "our labels:\n",
    " 0 - live\n",
    " 1 - attack\n",
    "\n",
    " oulumetics:\n",
    " 1 - live\n",
    " 0 - attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9.9992e-01, 1.0000e+00, 9.9984e-01,  ..., 1.1921e-07, 7.1526e-07,\n",
      "        2.3842e-07])\n"
     ]
    }
   ],
   "source": [
    "subtracted = 1 - test_model_op_cat_second_column_cpu\n",
    "\n",
    "print(subtracted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax with Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.006944444444444444\n",
      "0.002777777777777778\n",
      "0.004861111111111111\n"
     ]
    }
   ],
   "source": [
    "# softmax with threshould\n",
    "\n",
    "import oulumetrics\n",
    "\n",
    "apcer, bpcer, acer = oulumetrics.calculate_metrics(test_cat_labels_cpu, subtracted, 1-0.9293493032455444)\n",
    "\n",
    "print(apcer)\n",
    "print(bpcer)\n",
    "print(acer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004166666666666667\n",
      "0.008333333333333333\n",
      "0.00625\n"
     ]
    }
   ],
   "source": [
    "#  with argmax\n",
    "\n",
    "import oulumetrics\n",
    "\n",
    "\n",
    "apcer, bpcer, acer = oulumetrics.calculate_metrics(test_cat_labels_cpu, 1-test_predicted_cat_labels_cpu)\n",
    "print(apcer)\n",
    "print(bpcer)\n",
    "print(acer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt_fpad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
