{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment: pt_fpad  \n",
    "Python: 3.10.4     \n",
    "Pytorch: 2.1.1+cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To provide server to VPN_access via PC\n",
    "import os\n",
    "os.environ['http_proxy'] = 'http://10.162.15.186:5555'\n",
    "os.environ['https_proxy'] = 'http://10.162.15.186:5555'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/taha/anaconda3/envs/pt_fpad/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from torchvision import transforms, models\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_label_real = 0\n",
    "class_label_attack = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rose-Youtu Dataset:  \n",
    "Training -> 1397 (Real 358, Attack 1039) -> 1397/3495 * 100 = 40%  \n",
    "Validation -> 350 (Real 90, Attack 260) -> 350/3495 * 100 = 10%  \n",
    "Testing -> 1748 (Real 449, Attack 1299) -> 1748/3495 * 100 = 50%   \n",
    "Total = 3495"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_train_real = '/home/data/taha/FASdatasets/Rose_Youtu/train/real/'\n",
    "data_path_train_attack = '/home/data/taha/FASdatasets/Rose_Youtu/train/attack/'\n",
    "\n",
    "data_path_devel_real = '/home/data/taha/FASdatasets/Rose_Youtu/devel/real/'\n",
    "data_path_devel_attack = '/home/data/taha/FASdatasets/Rose_Youtu/devel/attack/'\n",
    "\n",
    "data_path_GAB_fb = '/home/taha/Taha26/All_Experiments/cyclegan_fpad_SRA_TRB_lsgan_res9/pytorch-CycleGAN-and-pix2pix-master-selftrain-fpad-RY/saved_results/RY_trainattack_GAB_fakebon6234'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_samples_image(image_path, class_label, transform):\n",
    "    image = Image.open(image_path).convert('RGB') # It uses PIL (Pillow) library to open the image, convert it to the RGB mode\n",
    "    sample = (transform(image), class_label) # Apply transformation\n",
    "    return sample\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, data_path, class_label):\n",
    "        self.data_path = data_path\n",
    "        self.image_files = [file for file in os.listdir(data_path) if file.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        self.class_label = class_label\n",
    "        self.data_length = len(self.image_files)\n",
    "        self.transform = transforms.Compose([transforms.Resize((256, 256)),\n",
    "                                             transforms.ToTensor()])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file = self.image_files[idx]\n",
    "        path = os.path.join(self.data_path, file)\n",
    "        sample = load_samples_image(path, self.class_label, self.transform)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_samples(path, class_label, transform): #Select N frames returned from read_all_frames and assign labels to all samples of same class\n",
    "        frames = read_all_frames(path)\n",
    "        total_frames = list(range(0, frames.shape[0], 1))\n",
    "        selected_samples = random.sample(total_frames, 1)\n",
    "        samples =[]\n",
    "        # Assign the same class label to all samples\n",
    "        label = class_label\n",
    "        samples =(transform(frames[selected_samples].squeeze()), label)     \n",
    "        return samples\n",
    "\n",
    "def read_all_frames(video_path): # reads all frames from a particular video and converts them to PyTorch tensors.\n",
    "    frame_list = []\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    success = True\n",
    "    while success:\n",
    "        success, frame = video.read()\n",
    "        if success:\n",
    "            frame = cv2.resize(frame, (256, 256), interpolation=cv2.INTER_AREA) #framesize kept 40, 30 as mentioned in paper but 224, 224 is also fine \n",
    "            frame_list.append(frame)\n",
    "    frame_list = np.array(frame_list)\n",
    "    return frame_list\n",
    "\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, data_path, class_label):\n",
    "        self.data_path = data_path #path for directory containing video files\n",
    "        self.video_files = [file for file in os.listdir(data_path) if file.endswith('.mp4')] #list of video files in the specified directory #.mov for RA and RM, .mp4 for RY\n",
    "        self.class_label = class_label #manually assign class_label for your desired class while loading\n",
    "        self.data_length = len(self.video_files) \n",
    "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    def __len__(self): # returns the total number of samples in the dataset\n",
    "        return self.data_length\n",
    "\n",
    "    def __getitem__(self, idx): # loads and returns a sample from the dataset at the given index\n",
    "        file = self.video_files[idx]\n",
    "        path = os.path.join(self.data_path, file)\n",
    "        frames= load_samples(path, self.class_label, self.transform)\n",
    "\n",
    "        return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_real = VideoDataset(data_path_train_real, class_label_real)\n",
    "train_dataset_attack = VideoDataset(data_path_train_attack, class_label_attack)\n",
    "\n",
    "val_dataset_real = VideoDataset(data_path_devel_real, class_label_real)\n",
    "val_dataset_attack = VideoDataset(data_path_devel_attack, class_label_attack)\n",
    "\n",
    "train_dataset_GAB_fb = ImageDataset(data_path_GAB_fb, class_label_attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_real = DataLoader(train_dataset_real, batch_size=1, shuffle=True)\n",
    "train_loader_attack = DataLoader(train_dataset_attack, batch_size=1, shuffle=True)\n",
    "\n",
    "val_loader_real = DataLoader(val_dataset_real, batch_size=1, shuffle=False)\n",
    "val_loader_attack = DataLoader(val_dataset_attack, batch_size=1, shuffle=False)\n",
    "\n",
    "train_loader_GAB_fb = DataLoader(train_dataset_GAB_fb, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_train_dataset = ConcatDataset([train_dataset_real, train_dataset_attack, train_dataset_GAB_fb])\n",
    "concatenated_val_dataset = ConcatDataset([val_dataset_real, val_dataset_attack])\n",
    "\n",
    "concatenated_train_loader = DataLoader(concatenated_train_dataset, batch_size=64, shuffle=True, pin_memory=True, num_workers=8)\n",
    "concatenated_val_loader = DataLoader(concatenated_val_dataset, batch_size=64, shuffle=False, pin_memory=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 7623\n",
      "Validation set size: 359\n"
     ]
    }
   ],
   "source": [
    "# Print dataset sizes\n",
    "print(f\"Training set size: {len(concatenated_train_dataset)}\")\n",
    "print(f\"Validation set size: {len(concatenated_val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileViTV2ForImageClassification(\n",
      "  (mobilevitv2): MobileViTV2Model(\n",
      "    (conv_stem): MobileViTV2ConvLayer(\n",
      "      (convolution): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (normalization): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activation): SiLU()\n",
      "    )\n",
      "    (encoder): MobileViTV2Encoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): MobileViTV2MobileNetLayer(\n",
      "          (layer): ModuleList(\n",
      "            (0): MobileViTV2InvertedResidual(\n",
      "              (expand_1x1): MobileViTV2ConvLayer(\n",
      "                (convolution): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (activation): SiLU()\n",
      "              )\n",
      "              (conv_3x3): MobileViTV2ConvLayer(\n",
      "                (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
      "                (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (activation): SiLU()\n",
      "              )\n",
      "              (reduce_1x1): MobileViTV2ConvLayer(\n",
      "                (convolution): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): MobileViTV2MobileNetLayer(\n",
      "          (layer): ModuleList(\n",
      "            (0): MobileViTV2InvertedResidual(\n",
      "              (expand_1x1): MobileViTV2ConvLayer(\n",
      "                (convolution): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (activation): SiLU()\n",
      "              )\n",
      "              (conv_3x3): MobileViTV2ConvLayer(\n",
      "                (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
      "                (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (activation): SiLU()\n",
      "              )\n",
      "              (reduce_1x1): MobileViTV2ConvLayer(\n",
      "                (convolution): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (1): MobileViTV2InvertedResidual(\n",
      "              (expand_1x1): MobileViTV2ConvLayer(\n",
      "                (convolution): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (activation): SiLU()\n",
      "              )\n",
      "              (conv_3x3): MobileViTV2ConvLayer(\n",
      "                (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
      "                (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (activation): SiLU()\n",
      "              )\n",
      "              (reduce_1x1): MobileViTV2ConvLayer(\n",
      "                (convolution): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): MobileViTV2Layer(\n",
      "          (downsampling_layer): MobileViTV2InvertedResidual(\n",
      "            (expand_1x1): MobileViTV2ConvLayer(\n",
      "              (convolution): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (activation): SiLU()\n",
      "            )\n",
      "            (conv_3x3): MobileViTV2ConvLayer(\n",
      "              (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
      "              (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (activation): SiLU()\n",
      "            )\n",
      "            (reduce_1x1): MobileViTV2ConvLayer(\n",
      "              (convolution): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (conv_kxk): MobileViTV2ConvLayer(\n",
      "            (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
      "            (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activation): SiLU()\n",
      "          )\n",
      "          (conv_1x1): MobileViTV2ConvLayer(\n",
      "            (convolution): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "          (transformer): MobileViTV2Transformer(\n",
      "            (layer): ModuleList(\n",
      "              (0-1): 2 x MobileViTV2TransformerLayer(\n",
      "                (layernorm_before): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
      "                (attention): MobileViTV2LinearSelfAttention(\n",
      "                  (qkv_proj): MobileViTV2ConvLayer(\n",
      "                    (convolution): Conv2d(128, 257, kernel_size=(1, 1), stride=(1, 1))\n",
      "                  )\n",
      "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "                  (out_proj): MobileViTV2ConvLayer(\n",
      "                    (convolution): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (dropout1): Dropout(p=0.0, inplace=False)\n",
      "                (layernorm_after): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
      "                (ffn): MobileViTV2FFN(\n",
      "                  (conv1): MobileViTV2ConvLayer(\n",
      "                    (convolution): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "                    (activation): SiLU()\n",
      "                  )\n",
      "                  (dropout1): Dropout(p=0.0, inplace=False)\n",
      "                  (conv2): MobileViTV2ConvLayer(\n",
      "                    (convolution): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "                  )\n",
      "                  (dropout2): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (layernorm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
      "          (conv_projection): MobileViTV2ConvLayer(\n",
      "            (convolution): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (3): MobileViTV2Layer(\n",
      "          (downsampling_layer): MobileViTV2InvertedResidual(\n",
      "            (expand_1x1): MobileViTV2ConvLayer(\n",
      "              (convolution): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (activation): SiLU()\n",
      "            )\n",
      "            (conv_3x3): MobileViTV2ConvLayer(\n",
      "              (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
      "              (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (activation): SiLU()\n",
      "            )\n",
      "            (reduce_1x1): MobileViTV2ConvLayer(\n",
      "              (convolution): Conv2d(512, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (normalization): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (conv_kxk): MobileViTV2ConvLayer(\n",
      "            (convolution): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "            (normalization): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activation): SiLU()\n",
      "          )\n",
      "          (conv_1x1): MobileViTV2ConvLayer(\n",
      "            (convolution): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "          (transformer): MobileViTV2Transformer(\n",
      "            (layer): ModuleList(\n",
      "              (0-3): 4 x MobileViTV2TransformerLayer(\n",
      "                (layernorm_before): GroupNorm(1, 192, eps=1e-05, affine=True)\n",
      "                (attention): MobileViTV2LinearSelfAttention(\n",
      "                  (qkv_proj): MobileViTV2ConvLayer(\n",
      "                    (convolution): Conv2d(192, 385, kernel_size=(1, 1), stride=(1, 1))\n",
      "                  )\n",
      "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "                  (out_proj): MobileViTV2ConvLayer(\n",
      "                    (convolution): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (dropout1): Dropout(p=0.0, inplace=False)\n",
      "                (layernorm_after): GroupNorm(1, 192, eps=1e-05, affine=True)\n",
      "                (ffn): MobileViTV2FFN(\n",
      "                  (conv1): MobileViTV2ConvLayer(\n",
      "                    (convolution): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "                    (activation): SiLU()\n",
      "                  )\n",
      "                  (dropout1): Dropout(p=0.0, inplace=False)\n",
      "                  (conv2): MobileViTV2ConvLayer(\n",
      "                    (convolution): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "                  )\n",
      "                  (dropout2): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (layernorm): GroupNorm(1, 192, eps=1e-05, affine=True)\n",
      "          (conv_projection): MobileViTV2ConvLayer(\n",
      "            (convolution): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (normalization): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (4): MobileViTV2Layer(\n",
      "          (downsampling_layer): MobileViTV2InvertedResidual(\n",
      "            (expand_1x1): MobileViTV2ConvLayer(\n",
      "              (convolution): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (normalization): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (activation): SiLU()\n",
      "            )\n",
      "            (conv_3x3): MobileViTV2ConvLayer(\n",
      "              (convolution): Conv2d(768, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=768, bias=False)\n",
      "              (normalization): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (activation): SiLU()\n",
      "            )\n",
      "            (reduce_1x1): MobileViTV2ConvLayer(\n",
      "              (convolution): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (conv_kxk): MobileViTV2ConvLayer(\n",
      "            (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "            (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activation): SiLU()\n",
      "          )\n",
      "          (conv_1x1): MobileViTV2ConvLayer(\n",
      "            (convolution): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          )\n",
      "          (transformer): MobileViTV2Transformer(\n",
      "            (layer): ModuleList(\n",
      "              (0-2): 3 x MobileViTV2TransformerLayer(\n",
      "                (layernorm_before): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
      "                (attention): MobileViTV2LinearSelfAttention(\n",
      "                  (qkv_proj): MobileViTV2ConvLayer(\n",
      "                    (convolution): Conv2d(256, 513, kernel_size=(1, 1), stride=(1, 1))\n",
      "                  )\n",
      "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "                  (out_proj): MobileViTV2ConvLayer(\n",
      "                    (convolution): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (dropout1): Dropout(p=0.0, inplace=False)\n",
      "                (layernorm_after): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
      "                (ffn): MobileViTV2FFN(\n",
      "                  (conv1): MobileViTV2ConvLayer(\n",
      "                    (convolution): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "                    (activation): SiLU()\n",
      "                  )\n",
      "                  (dropout1): Dropout(p=0.0, inplace=False)\n",
      "                  (conv2): MobileViTV2ConvLayer(\n",
      "                    (convolution): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "                  )\n",
      "                  (dropout2): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (layernorm): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
      "          (conv_projection): MobileViTV2ConvLayer(\n",
      "            (convolution): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/taha/anaconda3/envs/pt_fpad/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from transformers import MobileViTV2ForImageClassification\n",
    "model = MobileViTV2ForImageClassification.from_pretrained(\"apple/mobilevitv2-1.0-imagenet1k-256\")\n",
    "model.classifier = nn.Linear(in_features=512, out_features=2)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Training Loss: 0.1412, Training Accuracy: 97.31%, Validation Loss:  0.0562, Best Loss:  0.0562, Validation Accuracy: 96.94%\n",
      "Epoch 2/50, Training Loss: 0.0068, Training Accuracy: 99.90%, Validation Loss:  0.2603, Best Loss:  0.0562, Validation Accuracy: 92.20%\n",
      "Epoch 3/50, Training Loss: 0.0062, Training Accuracy: 99.82%, Validation Loss:  0.0063, Best Loss:  0.0063, Validation Accuracy: 100.00%\n",
      "Epoch 4/50, Training Loss: 0.0006, Training Accuracy: 100.00%, Validation Loss:  0.0169, Best Loss:  0.0063, Validation Accuracy: 98.89%\n",
      "Epoch 5/50, Training Loss: 0.0046, Training Accuracy: 99.86%, Validation Loss:  0.1615, Best Loss:  0.0063, Validation Accuracy: 95.54%\n",
      "Epoch 6/50, Training Loss: 0.0014, Training Accuracy: 99.96%, Validation Loss:  0.1190, Best Loss:  0.0063, Validation Accuracy: 96.94%\n",
      "Epoch 7/50, Training Loss: 0.0068, Training Accuracy: 99.78%, Validation Loss:  0.0316, Best Loss:  0.0063, Validation Accuracy: 98.61%\n",
      "Early stopping at epoch 7\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Set up early stopping parameters\n",
    "patience = 5  # Number of epochs with no improvement after which training will be stopped\n",
    "best_loss = float('inf') #set to positive infinity to ensure that the first validation loss encountered will always be considered an improvement\n",
    "counter = 0  # Counter to keep track of consecutive epochs with no improvement\n",
    "\n",
    "#Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    train_correct_predictions = 0\n",
    "    total_train_samples = 0\n",
    "\n",
    "    for train_images, train_labels in concatenated_train_loader:\n",
    "        train_images, train_labels = train_images.to(device), train_labels.to(device)\n",
    "        # Clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward Pass\n",
    "        train_outputs = model(train_images)\n",
    "\n",
    "        # Extract logits\n",
    "        train_logits = train_outputs.logits\n",
    "\n",
    "        # Find the Loss\n",
    "        train_loss = criterion(train_logits, train_labels)\n",
    "        # Calculate gradients\n",
    "        train_loss.backward()\n",
    "        # Update Weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # accumulate the training loss\n",
    "        running_loss += train_loss.item()\n",
    "\n",
    "        # calculate training accuracy\n",
    "        _, train_predicted = torch.max(train_logits, 1) # _ contain max value, train_predicted contain the indices where maximum value occured\n",
    "        train_correct_predictions += (train_predicted == train_labels).sum().item() \n",
    "        total_train_samples += train_labels.size(0)\n",
    "            \n",
    "    train_total_loss = running_loss / len(concatenated_train_loader)\n",
    "    train_accuracy = train_correct_predictions / total_train_samples * 100\n",
    "    train_losses.append(train_total_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    val_running_loss = 0.0\n",
    "    val_correct_prediction = 0\n",
    "    total_val_samples = 0\n",
    "\n",
    "    #Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for val_images, val_labels in concatenated_val_loader:\n",
    "            val_images, val_labels = val_images.to(device), val_labels.to(device)\n",
    "            val_op = model(val_images)\n",
    "\n",
    "            # Assuming val_op is the model's output, which is of type ImageClassifierOutputWithNoAttention\n",
    "            val_logits = val_op.logits\n",
    "\n",
    "            val_loss = criterion(val_logits, val_labels)\n",
    "            val_running_loss += val_loss.item()\n",
    "\n",
    "            _, val_predicted = torch.max(val_logits, 1)\n",
    "            val_correct_prediction += (val_predicted == val_labels).sum().item()\n",
    "            total_val_samples += val_labels.size(0)\n",
    "        \n",
    "        val_total_loss = val_running_loss / len(concatenated_val_loader)\n",
    "        val_accuracy = val_correct_prediction / total_val_samples * 100\n",
    "        val_losses.append(val_total_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "    # Check if validation loss has improved\n",
    "    if val_total_loss < best_loss:\n",
    "        best_loss = val_total_loss\n",
    "        counter = 0\n",
    "        # Save the model if needed\n",
    "        torch.save(model.state_dict(), 'mobilevitv2_RY_GAB_FB.pth')\n",
    "\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "        # Check if training should be stopped\n",
    "        if counter >= patience:\n",
    "            print(f'Early stopping at epoch {epoch}')\n",
    "            break\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Training Loss: {train_total_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%, Validation Loss: {val_total_loss: .4f}, Best Loss: {best_loss: .4f}, Validation Accuracy: {val_accuracy:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt_fpad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
